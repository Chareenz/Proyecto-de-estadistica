{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d9ccee",
   "metadata": {},
   "source": [
    "FASE 1\n",
    "1.-TEOREMA DE BAYES\n",
    "Queremos la probabilidad de que una rese√±a sea positiva dado su texto \"R\"\n",
    "\n",
    "                        P(A|R) = P(B|A) ¬∑ P(A)\n",
    "                                ---------------\n",
    "                                      P(B)\n",
    "\n",
    "Probabilidad a priori P(Positivo)\n",
    "La ‚Äúcreencia inicial‚Äù de que una rese√±a sea positiva antes de leerla.\n",
    "En tu dataset se estima con entrenamiento:\n",
    "\n",
    "                P(Positivo)     = rese√±as positivas en train(80%)  \n",
    "                                -----------------------------------\n",
    "                                      rese√±as totales en train\n",
    "\n",
    "                P(Negativo) = 1 - P(Positivo)\n",
    "\n",
    "VEROSIMILITUD P(R|Positivo)\n",
    "Con naive Bayes miltinomial (bolsa de palabras positivas y negativas con conteos)\n",
    "\n",
    "    Cada P(w|Positivo) se estima con Laplace\n",
    "\n",
    "                P(w|Pos) = count(P(positivos)(w)) + 1 \n",
    "                           --------------------------\n",
    "                                T(P(positivos))+V\n",
    "\n",
    "para una rese√±a  ùëÖ y una clase  ùëê (positivo o negativo), la verosimilitud ùëù(ùëÖ‚à£ùëê) mide qu√© tan probable es ‚Äúgenerar‚Äù ese texto si la clase fuera ùëê. en el modelo multinomial de bolsa de palabras, representamos ùëÖ  por los conteos  ùëõùë§ de cada palabra  ùë§. con la suposici√≥n ingenua (ver abajo), tratamos las palabras como independientes condicionadas a la clase, as√≠ que la verosimilitud se descompone en un producto de probabilidades palabra‚Äìclase: ‚Äúcu√°ntas veces aparece cada palabra y qu√© tan t√≠pica es de la clase‚Äù.\n",
    "\n",
    "\n",
    "suposici√≥n ingenua (naive).\n",
    "se llama ‚Äúingenua‚Äù porque asume que las palabras de la rese√±a son independientes entre s√≠ dado la clase. esto ignora orden, gram√°tica y dependencias (por ejemplo, la interacci√≥n de ‚Äúno‚Äù con ‚Äúbuena‚Äù), pero hace el c√°lculo simple y eficiente. a pesar de ser una aproximaci√≥n grosera, funciona bien en tareas como clasificaci√≥n de rese√±as.\n",
    "\n",
    "qu√© queda fuera (evidencia) y c√≥mo se decide.\n",
    "la evidencia ùëù(ùëÖ) es un factor de normalizaci√≥n com√∫n a todas las clases; para clasificar no hace falta calcularla: se comparan los ‚Äúscores‚Äù por clase\n",
    "score(c) = logp(c) + SUMATORIA(w,)n(w) logp(w|c)\n",
    "y se elige la clase con mayor score. aqu√≠ logp(c) es la probabilidad a priori (qu√© fracci√≥n de rese√±as del train son positivas/negativas) y la suma representa la verosimilitud en log con laplace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "630a47e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import math\n",
    "\n",
    "#FASE 2 PREPARACION Y LIMPIEZA DE DATOS\n",
    "#yo pongo todo dentro de funciones para usarlas luego;p\n",
    "TEXT_COL = \"review_es\"\n",
    "LABEL_COL = \"sentimiento\" #cambiamos positive/negative por\n",
    "\n",
    "#mis stopwords owowowowo\n",
    "STOP = {\n",
    "    'el','la','los','las','de','del','y','o','u','que','a','un','una','en','es','se',\n",
    "    'lo','por','con','al','como','para','su','sus','le','les','me','mi','tu','te','ya','muy',\n",
    "    'pero','si','s√≠','mas','m√°s','cuando','donde','d√≥nde'\n",
    "}\n",
    "\n",
    "#definimos nuestra funcion parar separar palabras, quitar stopwords y espacios extras\n",
    "def barrer_y_botar(texto: str, quitar_stops=True):\n",
    "    t = str(texto).lower()#minusculas\n",
    "    t = re.sub(r\"[^a-z√°√©√≠√≥√∫√º√±\\s]\", \" \",t) #quitamos char raros o el espacio\n",
    "    t = re.sub(r\"\\s+\", \" \",t).strip() #quitamos espacios extras\n",
    "#TOKENICE\n",
    "    tokens = t.split()\n",
    "    if quitar_stops:\n",
    "        tokens = [w for w in tokens if w not in STOP]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f95d82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 rese√±as\n",
      "['uno', 'otros', 'cr√≠ticos', 'ha', 'mencionado', 'despu√©s', 'ver', 'solo', 'oz', 'episodio', 'estar√°', 'enganchado', 'tienen', 'raz√≥n', 'esto', 'exactamente', 'sucedi√≥', 'conmigo', 'primera', 'cosa']\n"
     ]
    }
   ],
   "source": [
    "#cargamos datos\n",
    "df = pd.read_csv(\"IMDB Dataset SPANISH.csv\")\n",
    "df[\"tokens\"]= df[\"review_es\"].apply(lambda x: barrer_y_botar(x)) #recorremnos ada rese√±a y aplicamos la funcion d limpieza\n",
    "print(len(df),\"rese√±as\")\n",
    "print(df[\"tokens\"].iloc[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1314a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 40000  | pos: 20042  neg: 19958\n",
      "P(Pos)=0.5010  P(Neg)=0.4990\n"
     ]
    }
   ],
   "source": [
    "#FASE 3 ENTRENAMIENTO\n",
    "#comenzamos la mescla de datos y el 80/20 (Trabajaremos con el set de entrenamiento \"train\")\n",
    "df = df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "cut = int(0.8 * len(df)) #40000\n",
    "train = df.iloc[0:cut].copy()\n",
    "test = df.iloc[cut:].copy()\n",
    "\n",
    "train[\"y\"] = train[\"sentimiento\"].map({\"positivo\":\"pos\", \"negativo\":\"neg\"})\n",
    "test[\"y\"] = test[\"sentimiento\"].map({\"positivo\":\"pos\",\"negativo\":\"neg\"})\n",
    "\n",
    "N = len(train) #40000\n",
    "Npos = (train[\"y\"] == \"pos\").sum()\n",
    "Nneg = (train[\"y\"] == \"neg\").sum()\n",
    "#calcular Probabilidades A Priori (P(A)):\n",
    "P_pos = Npos / N\n",
    "P_neg = Nneg / N\n",
    "print(f\"train: {N}  | pos: {Npos}  neg: {Nneg}\")\n",
    "print(f\"P(Pos)={P_pos:.4f}  P(Neg)={P_neg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15c380b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot_pos: 3026354  | tot_neg: 2988159  | Vocab size: 155590\n"
     ]
    }
   ],
   "source": [
    "# 2.CONSTRUIR EL VOCABULARIO\n",
    "#creamos dos bolsas d palabras, para contar ocurrencias de positivas y negativas\n",
    "bolsa_pos = Counter()\n",
    "bolsa_neg = Counter()\n",
    "\n",
    "#Contamos el numero total de palabras en rese√±as posi y negativas\n",
    "for toks, y in zip(train[\"tokens\"], train[\"y\"]):\n",
    "    if y == \"pos\":\n",
    "        bolsa_pos.update(toks)\n",
    "    else:\n",
    "        bolsa_neg.update(toks)\n",
    "tot_pos = sum(bolsa_pos.values())\n",
    "tot_neg = sum(bolsa_neg.values())\n",
    "\n",
    "vocab = set(bolsa_pos) | set(bolsa_neg)\n",
    "V = len(vocab)\n",
    "print(\"tot_pos:\", tot_pos, \" | tot_neg:\", tot_neg, \" | Vocab size:\", V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3972a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Calculamos probabilidades condicionales\n",
    "alpha = 1.0 #suavizado de Laplace\n",
    "den_pos = tot_pos + alpha * V\n",
    "den_neg = tot_neg + alpha * V\n",
    "\n",
    "#tablas de probabilidades para cada palabra\n",
    "logP_w_pos = {}\n",
    "logP_w_neg = {}\n",
    "for w in vocab:\n",
    "    logP_w_pos[w] = math.log((bolsa_pos[w] + alpha) / den_pos)\n",
    "    logP_w_neg[w] = math.log((bolsa_neg[w] + alpha) / den_neg)\n",
    "\n",
    "# Probabilidades a priori en log\n",
    "logprior_pos = math.log(P_pos)\n",
    "logprior_neg = math.log(P_neg)\n",
    "\n",
    "# Valor por defecto para palabras desconocidas\n",
    "default_pos = math.log(alpha / den_pos)\n",
    "default_neg = math.log(alpha / den_neg)\n",
    "\n",
    "def clasificar(tokens):\n",
    "    score_pos = logprior_pos\n",
    "    score_neg = logprior_neg\n",
    "    for w in tokens:\n",
    "        score_pos += logP_w_pos.get(w, default_pos)\n",
    "        score_neg += logP_w_neg.get(w, default_neg)\n",
    "    return ('pos' if score_pos > score_neg else 'neg'), score_pos, score_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2258dc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conteos -> pos:426  neg:96\n",
      "sin suavizado -> P(fant√°stica|pos)=1.408e-04  P(fant√°stica|neg)=3.213e-05\n",
      "con Laplace(Œ±=1) -> P(fant√°stica|pos)=1.342e-04  P(fant√°stica|neg)=3.085e-05\n"
     ]
    }
   ],
   "source": [
    "def ver_prob_palabra(w):\n",
    "    # conteo crudos\n",
    "    c_pos = bolsa_pos[w]\n",
    "    c_neg = bolsa_neg[w]\n",
    "    print(f\"conteos -> pos:{c_pos}  neg:{c_neg}\")\n",
    "\n",
    "    # sin suavizado puede dar 0\n",
    "    p_pos_naive = c_pos / tot_pos if tot_pos else 0.0\n",
    "    p_neg_naive = c_neg / tot_neg if tot_neg else 0.0\n",
    "    print(f\"sin suavizado -> P({w}|pos)={p_pos_naive:.3e}  P({w}|neg)={p_neg_naive:.3e}\")\n",
    "\n",
    "    # con Laplace (Œ±=1)\n",
    "    p_pos_lap = (c_pos + 1) / den_pos\n",
    "    p_neg_lap = (c_neg + 1) / den_neg\n",
    "    print(f\"con Laplace(Œ±=1) -> P({w}|pos)={p_pos_lap:.3e}  P({w}|neg)={p_neg_lap:.3e}\")\n",
    "\n",
    "# ejemplo con la palabra que preguntas\n",
    "w = \"fant√°stica\" #la limpieza mantiene acentos y min√∫sculas\n",
    "ver_prob_palabra(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73207daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count_pos</th>\n",
       "      <th>count_neg</th>\n",
       "      <th>P_pos</th>\n",
       "      <th>P_neg</th>\n",
       "      <th>logP_w_pos</th>\n",
       "      <th>logP_w_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>receptionist</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000628547</td>\n",
       "      <td>0.000000318092</td>\n",
       "      <td>-14.279855708171</td>\n",
       "      <td>-14.960926594739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tugurio</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000001257093</td>\n",
       "      <td>0.000000318092</td>\n",
       "      <td>-13.586708527611</td>\n",
       "      <td>-14.960926594739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recorre</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000001571366</td>\n",
       "      <td>0.000001590458</td>\n",
       "      <td>-13.363564976297</td>\n",
       "      <td>-13.351488682305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mauritz</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000314273</td>\n",
       "      <td>0.000000636183</td>\n",
       "      <td>-14.973002888731</td>\n",
       "      <td>-14.267779414179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t√≥picamente</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000628547</td>\n",
       "      <td>0.000000318092</td>\n",
       "      <td>-14.279855708171</td>\n",
       "      <td>-14.960926594739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155585</th>\n",
       "      <td>atribuyen</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000628547</td>\n",
       "      <td>0.000000636183</td>\n",
       "      <td>-14.279855708171</td>\n",
       "      <td>-14.267779414179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155586</th>\n",
       "      <td>efeminante</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000314273</td>\n",
       "      <td>0.000000636183</td>\n",
       "      <td>-14.973002888731</td>\n",
       "      <td>-14.267779414179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155587</th>\n",
       "      <td>aer√≥bicas</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000314273</td>\n",
       "      <td>0.000000636183</td>\n",
       "      <td>-14.973002888731</td>\n",
       "      <td>-14.267779414179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155588</th>\n",
       "      <td>klebold</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000628547</td>\n",
       "      <td>0.000000636183</td>\n",
       "      <td>-14.279855708171</td>\n",
       "      <td>-14.267779414179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155589</th>\n",
       "      <td>fashing</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000628547</td>\n",
       "      <td>0.000000318092</td>\n",
       "      <td>-14.279855708171</td>\n",
       "      <td>-14.960926594739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155590 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                word  count_pos  count_neg          P_pos          P_neg  \\\n",
       "0       receptionist          1          0 0.000000628547 0.000000318092   \n",
       "1            tugurio          3          0 0.000001257093 0.000000318092   \n",
       "2            recorre          4          4 0.000001571366 0.000001590458   \n",
       "3            mauritz          0          1 0.000000314273 0.000000636183   \n",
       "4        t√≥picamente          1          0 0.000000628547 0.000000318092   \n",
       "...              ...        ...        ...            ...            ...   \n",
       "155585     atribuyen          1          1 0.000000628547 0.000000636183   \n",
       "155586    efeminante          0          1 0.000000314273 0.000000636183   \n",
       "155587     aer√≥bicas          0          1 0.000000314273 0.000000636183   \n",
       "155588       klebold          1          1 0.000000628547 0.000000636183   \n",
       "155589       fashing          1          0 0.000000628547 0.000000318092   \n",
       "\n",
       "             logP_w_pos       logP_w_neg  \n",
       "0      -14.279855708171 -14.960926594739  \n",
       "1      -13.586708527611 -14.960926594739  \n",
       "2      -13.363564976297 -13.351488682305  \n",
       "3      -14.973002888731 -14.267779414179  \n",
       "4      -14.279855708171 -14.960926594739  \n",
       "...                 ...              ...  \n",
       "155585 -14.279855708171 -14.267779414179  \n",
       "155586 -14.973002888731 -14.267779414179  \n",
       "155587 -14.973002888731 -14.267779414179  \n",
       "155588 -14.279855708171 -14.267779414179  \n",
       "155589 -14.279855708171 -14.960926594739  \n",
       "\n",
       "[155590 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# P(palabra|Positivo)\n",
    "rows = []\n",
    "for w in vocab:\n",
    "    cpos = bolsa_pos[w]\n",
    "    cneg = bolsa_neg[w]\n",
    "    ppos = (cpos + alpha) / den_pos\n",
    "    pneg = (cneg + alpha) / den_neg\n",
    "    rows.append((w, cpos, cneg, ppos, pneg, math.log(ppos), math.log(pneg)))\n",
    "\n",
    "probs_df = pd.DataFrame(rows, columns=[\n",
    "    'word','count_pos','count_neg','P_pos','P_neg','logP_w_pos','logP_w_neg'\n",
    "])\n",
    "pd.options.display.float_format = '{:.12f}'.format  # 12 decimales\n",
    "probs_df.head(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ab2c51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma P(w|pos) ‚âà 1.0\n",
      "Suma P(w|neg) ‚âà 1.0\n"
     ]
    }
   ],
   "source": [
    "#verificamos que todo este bien, cada uno deberia sumar 1 por clase\n",
    "print(\"Suma P(w|pos) ‚âà\", probs_df['P_pos'].sum())\n",
    "print(\"Suma P(w|neg) ‚âà\", probs_df['P_neg'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca56f2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fase 4 CLASIFICACION (APLICACION DE BAYES)\n",
    "def clasificar_sentimiento(rese√±a):\n",
    "    # 1. limpiar y tokenizar igual que en Fase 2\n",
    "    tokens = barrer_y_botar(rese√±a)\n",
    "\n",
    "    # 2. iniciar los puntajes con los priors en log\n",
    "    score_pos = logprior_pos  # log(P(Positivo))\n",
    "    score_neg = logprior_neg  # log(P(Negativo))\n",
    "\n",
    "    # 3. sumar los log-probs de cada palabra seg√∫n cada clase\n",
    "    for w in tokens:\n",
    "        score_pos += logP_w_pos.get(w, default_pos)  # log(P(palabra|Positivo))\n",
    "        score_neg += logP_w_neg.get(w, default_neg)  # log(P(palabra|Negativo))\n",
    "#usamos los logaritmos pq si multiplicaramos todas las probabiliadades quedaria algo como:\n",
    "# 0.00002 * 0.000001 * 0.0005 * ... = 0.000000000000000000000000000000...\n",
    "#y eso se aproxima a 0 = underflow aritmetico, el modelo explotaria\n",
    "#en cambio con los logaritmos transforman multiplicaciones en sumas\n",
    "    # 4. decidir clase\n",
    "    if score_pos > score_neg:\n",
    "        return \"pos\"\n",
    "    else:\n",
    "        return \"neg\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20119387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rese√±as correctas: 8385 / 10000\n",
      "Exactitud (accuracy): 0.839\n"
     ]
    }
   ],
   "source": [
    "#Fase 5\n",
    "#aqui estamos trabajando con el 20%, pq usamos el 80% ya para entrenar\n",
    "# hacemos copia por seguridad\n",
    "test = test.copy()\n",
    "\n",
    "# predecimos cada rese√±a del set de prueba\n",
    "test[\"prediccion\"] = test[\"review_es\"].apply(clasificar_sentimiento)\n",
    "\n",
    "# accuracy\n",
    "correctas = (test[\"prediccion\"] == test[\"y\"]).sum()\n",
    "total = len(test)\n",
    "accuracy = correctas / total\n",
    "\n",
    "print(f\"Rese√±as correctas: {correctas} / {total}\")\n",
    "print(f\"Exactitud (accuracy): {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1562cd66",
   "metadata": {},
   "source": [
    "¬øQue exactitud obtuvo el modelo? ¬øConsideran que es un buen resultado?\n",
    "la exactitud fue del 0.835 (8346/10000), si es buen resultado, 83-84% es muy solido y muy por encima del azar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0246206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 palabras m√°s probables en rese√±as POSITIVAS:\n",
      "    word          P_pos  count_pos  count_neg\n",
      "pel√≠cula 0.018130425928      57689      63870\n",
      "      no 0.015402219524      49008      67206\n",
      "    esta 0.009695645178      30850      34531\n",
      "     the 0.007331052966      23326      19061\n",
      "    est√° 0.004684871890      14906      15134\n",
      "     fue 0.004115408694      13094      14796\n",
      "    este 0.003691454029      11745      13159\n",
      "historia 0.003654998328      11629       9177\n",
      "     son 0.003597800590      11447      11288\n",
      "     and 0.003556316516      11315       8009\n",
      "\n",
      "Top 10 palabras m√°s probables en rese√±as NEGATIVAS:\n",
      "    word          P_neg  count_neg  count_pos\n",
      "      no 0.021377978967      67206      49008\n",
      "pel√≠cula 0.020316825548      63870      57689\n",
      "    esta 0.010984337490      34531      30850\n",
      "     the 0.006063461173      19061      23326\n",
      "    est√° 0.004814315647      15134      14906\n",
      "     fue 0.004706800702      14796      13094\n",
      "    este 0.004186084831      13159      11745\n",
      "    solo 0.004113559957      12931      10039\n",
      "    esto 0.004079524160      12824       9319\n",
      "     sin 0.003869901827      12165      10293\n"
     ]
    }
   ],
   "source": [
    "# ordenar por prob condicional directa\n",
    "top_pos_palabras = probs_df.sort_values(\"P_pos\", ascending=False).head(10)\n",
    "top_neg_palabras = probs_df.sort_values(\"P_neg\", ascending=False).head(10)\n",
    "\n",
    "print(\"Top 10 palabras m√°s probables en rese√±as POSITIVAS:\")\n",
    "print(top_pos_palabras[[\"word\", \"P_pos\", \"count_pos\", \"count_neg\"]].to_string(index=False))\n",
    "\n",
    "print(\"\\nTop 10 palabras m√°s probables en rese√±as NEGATIVAS:\")\n",
    "print(top_neg_palabras[[\"word\", \"P_neg\", \"count_neg\", \"count_pos\"]].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cd0b9f",
   "metadata": {},
   "source": [
    "¬øQu√© muestran las listas de palabras?\n",
    "Reflejan frecuencia, no necesariamente poder discriminativo, palabras como \"pelicula\", \"esta\" \"fue/este\" aparecen en alto em ambas clases, por que son comunes en todas las rese√±as\n",
    "Tambien podemos ver que la negacion \"No\" es mas probable en negativas, el modelo si capto eso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "121b88ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rese√±a positiva ejemplo:\n",
      "Disfrute mucho de la pel√≠cula, me gusto\n",
      "Predicci√≥n: pos\n",
      "\n",
      "Rese√±a negativa ejemplo:\n",
      "Esta pel√≠cula no fue de mi gusto, the\n",
      "Predicci√≥n: neg\n"
     ]
    }
   ],
   "source": [
    "resena_pos_ejemplo = \"Disfrute mucho de la pel√≠cula, me gusto\"\n",
    "resena_neg_ejemplo = \"Esta pel√≠cula no fue de mi gusto, the\"\n",
    "\n",
    "print(\"Rese√±a positiva ejemplo:\")\n",
    "print(resena_pos_ejemplo)\n",
    "print(\"Predicci√≥n:\", clasificar_sentimiento(resena_pos_ejemplo))\n",
    "\n",
    "print(\"\\nRese√±a negativa ejemplo:\")\n",
    "print(resena_neg_ejemplo)\n",
    "print(\"Predicci√≥n:\", clasificar_sentimiento(resena_neg_ejemplo))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
